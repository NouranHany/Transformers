{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Overview\n",
        "* This is an Arabic licence plate recognition system\n",
        "* 2 Main procedures were applied\n",
        "  * Preprocessing on plate\n",
        "  * Predicting the plate characters"
      ],
      "metadata": {
        "id": "cvgl_Qj3RHgp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "JU8aEy1zRHgu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install imutils\n",
        "!pip install tensorflow\n",
        "!pip install keras"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-25T18:03:02.650225Z",
          "iopub.execute_input": "2022-03-25T18:03:02.651105Z",
          "iopub.status.idle": "2022-03-25T18:03:02.656385Z",
          "shell.execute_reply.started": "2022-03-25T18:03:02.650971Z",
          "shell.execute_reply": "2022-03-25T18:03:02.655480Z"
        },
        "trusted": true,
        "id": "Sx3s1-JvRHgv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip './cropped_letters.zip'  -d './cropped_letters'\n",
        "!unzip './cropped_numbers.zip'  -d './cropped_numbers'\n",
        "!unzip './test_images.zip'  -d './test_images'"
      ],
      "metadata": {
        "id": "0x1JIhWcUy0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Including needed libraries"
      ],
      "metadata": {
        "id": "AYgue9W8Q4f2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "#Deep Learning\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers as L\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from skimage.transform import resize, rescale\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Dense, Flatten, MaxPooling2D, Dropout, Conv2D,BatchNormalization\n",
        "from imutils import paths\n",
        "from skimage.segmentation import clear_border\n",
        "from PIL import Image\n",
        "\n",
        "import argparse\n",
        "import imutils\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import skimage \n",
        "#from imutils.perspective import four_point_transform\n",
        "import os\n",
        "import cv2\n",
        "import csv\n",
        "import glob\n",
        "import skimage.io as io\n",
        "import pickle\n",
        "from skimage.color import rgb2gray, gray2rgb\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-25T18:03:04.089541Z",
          "iopub.execute_input": "2022-03-25T18:03:04.090035Z",
          "iopub.status.idle": "2022-03-25T18:03:06.244937Z",
          "shell.execute_reply.started": "2022-03-25T18:03:04.089993Z",
          "shell.execute_reply": "2022-03-25T18:03:06.244126Z"
        },
        "trusted": true,
        "id": "OS7UPLmpRHgw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining needed preprocessing functions."
      ],
      "metadata": {
        "id": "vjEiHO_PQ-ST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# module level variables ##########################################################################\n",
        "GAUSSIAN_SMOOTH_FILTER_SIZE = (5, 5)\n",
        "ADAPTIVE_THRESH_BLOCK_SIZE = 19\n",
        "ADAPTIVE_THRESH_WEIGHT = 9\n",
        "\n",
        "###################################################################################################\n",
        "def preprocess(imgOriginal):\n",
        "    imgSaturation, imgGrayscale = extractValue(imgOriginal)\n",
        "\n",
        "    imgMaxContrastGrayscale = maximizeContrast(imgGrayscale)\n",
        "\n",
        "    imgBlurred = cv2.GaussianBlur(imgMaxContrastGrayscale, GAUSSIAN_SMOOTH_FILTER_SIZE, 0)\n",
        "\n",
        "    imgThreshValue = cv2.threshold(imgBlurred, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]\n",
        "    imgThreshSaturation = cv2.threshold(imgSaturation, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]\n",
        "\n",
        "    # plt.imshow(imgBlurred,cmap='gray')\n",
        "    # plt.show()\n",
        "    # plt.imshow(imgThreshValue,cmap='gray')\n",
        "    # plt.show()\n",
        "\n",
        "    # plt.imshow(imgSaturation,cmap='gray')\n",
        "    # plt.show()\n",
        "    # plt.imshow(imgThreshSaturation,cmap='gray')\n",
        "    # plt.show()\n",
        "\n",
        "    #imgThresh = cv2.adaptiveThreshold(imgBlurred, 255.0, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, ADAPTIVE_THRESH_BLOCK_SIZE, ADAPTIVE_THRESH_WEIGHT)\n",
        "    \n",
        "    return imgGrayscale, (imgThreshValue & (255 - imgThreshSaturation))\n",
        "# end function\n",
        "\n",
        "def preprocess2(imgOriginal):\n",
        "    _, imgGrayscale = extractValue(imgOriginal)\n",
        "\n",
        "    imgBlurred = cv2.GaussianBlur(imgGrayscale, GAUSSIAN_SMOOTH_FILTER_SIZE, 0)\n",
        "\n",
        "    imgThreshValue = cv2.threshold(imgBlurred, 0, 255, cv2.THRESH_OTSU)[1]\n",
        "\n",
        "    # plt.imshow(imgBlurred,cmap='gray')\n",
        "    # plt.show()\n",
        "    # plt.imshow(imgThreshValue,cmap='gray')\n",
        "    # plt.show()\n",
        "\n",
        "    # plt.imshow(imgSaturation,cmap='gray')\n",
        "    # plt.show()\n",
        "    # plt.imshow(imgThreshSaturation,cmap='gray')\n",
        "    # plt.show()\n",
        "\n",
        "    #imgThresh = cv2.adaptiveThreshold(imgBlurred, 255.0, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, ADAPTIVE_THRESH_BLOCK_SIZE, ADAPTIVE_THRESH_WEIGHT)\n",
        "    \n",
        "    return imgThreshValue\n",
        "# end function\n",
        "\n",
        "###################################################################################################\n",
        "def extractValue(imgOriginal):\n",
        "    height, width, numChannels = imgOriginal.shape\n",
        "\n",
        "    imgHSV = np.zeros((height, width, 3), np.uint8)\n",
        "\n",
        "    imgHSV = cv2.cvtColor(imgOriginal, cv2.COLOR_BGR2HSV)\n",
        "\n",
        "    imgHue, imgSaturation, imgValue = cv2.split(imgHSV)\n",
        "\n",
        "    return imgSaturation, imgValue\n",
        "# end function\n",
        "\n",
        "###################################################################################################\n",
        "def maximizeContrast(imgGrayscale):\n",
        "\n",
        "    height, width = imgGrayscale.shape\n",
        "\n",
        "    imgTopHat = np.zeros((height, width, 1), np.uint8)\n",
        "    imgBlackHat = np.zeros((height, width, 1), np.uint8)\n",
        "\n",
        "    structuringElement = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n",
        "\n",
        "    imgTopHat = cv2.morphologyEx(imgGrayscale, cv2.MORPH_TOPHAT, structuringElement)\n",
        "    imgBlackHat = cv2.morphologyEx(imgGrayscale, cv2.MORPH_BLACKHAT, structuringElement)\n",
        "\n",
        "    imgGrayscalePlusTopHat = cv2.add(imgGrayscale, imgTopHat)\n",
        "    imgGrayscalePlusTopHatMinusBlackHat = cv2.subtract(imgGrayscalePlusTopHat, imgBlackHat)\n",
        "\n",
        "    return imgGrayscalePlusTopHatMinusBlackHat\n",
        "# end function"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-25T18:03:10.329095Z",
          "iopub.execute_input": "2022-03-25T18:03:10.329709Z",
          "iopub.status.idle": "2022-03-25T18:03:10.344251Z",
          "shell.execute_reply.started": "2022-03-25T18:03:10.329667Z",
          "shell.execute_reply": "2022-03-25T18:03:10.341504Z"
        },
        "trusted": true,
        "id": "fH_XqP93RHgy"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def order_points(pts):\n",
        "\t# initialzie a list of coordinates that will be ordered\n",
        "\t# such that the first entry in the list is the top-left,\n",
        "\t# the second entry is the top-right, the third is the\n",
        "\t# bottom-right, and the fourth is the bottom-left\n",
        "\trect = np.zeros((4, 2), dtype = \"float32\")\n",
        "\t# the top-left point will have the smallest sum, whereas\n",
        "\t# the bottom-right point will have the largest sum\n",
        "\ts = pts.sum(axis = 1)\n",
        "\trect[0] = pts[np.argmin(s)]\n",
        "\trect[2] = pts[np.argmax(s)]\n",
        "\t# now, compute the difference between the points, the\n",
        "\t# top-right point will have the smallest difference,\n",
        "\t# whereas the bottom-left will have the largest difference\n",
        "\tdiff = np.diff(pts, axis = 1)\n",
        "\trect[1] = pts[np.argmin(diff)]\n",
        "\trect[3] = pts[np.argmax(diff)]\n",
        "\t# return the ordered coordinates\n",
        "\treturn rect"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-25T18:03:12.712762Z",
          "iopub.execute_input": "2022-03-25T18:03:12.713042Z",
          "iopub.status.idle": "2022-03-25T18:03:12.719320Z",
          "shell.execute_reply.started": "2022-03-25T18:03:12.713011Z",
          "shell.execute_reply": "2022-03-25T18:03:12.718512Z"
        },
        "trusted": true,
        "id": "fgGXn6OGRHg0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def four_point_transform(image, pts):\n",
        "\t# obtain a consistent order of the points and unpack them\n",
        "\t# individually\n",
        "\trect = order_points(pts)\n",
        "\t(tl, tr, br, bl) = rect\n",
        "\t# compute the width of the new image, which will be the\n",
        "\t# maximum distance between bottom-right and bottom-left\n",
        "\t# x-coordiates or the top-right and top-left x-coordinates\n",
        "\twidthA = np.sqrt(((br[0] - bl[0]) ** 2) + ((br[1] - bl[1]) ** 2))\n",
        "\twidthB = np.sqrt(((tr[0] - tl[0]) ** 2) + ((tr[1] - tl[1]) ** 2))\n",
        "\tmaxWidth = max(int(widthA), int(widthB))\n",
        "\t# compute the height of the new image, which will be the\n",
        "\t# maximum distance between the top-right and bottom-right\n",
        "\t# y-coordinates or the top-left and bottom-left y-coordinates\n",
        "\theightA = np.sqrt(((tr[0] - br[0]) ** 2) + ((tr[1] - br[1]) ** 2))\n",
        "\theightB = np.sqrt(((tl[0] - bl[0]) ** 2) + ((tl[1] - bl[1]) ** 2))\n",
        "\tmaxHeight = max(int(heightA), int(heightB))\n",
        "\t# now that we have the dimensions of the new image, construct\n",
        "\t# the set of destination points to obtain a \"birds eye view\",\n",
        "\t# (i.e. top-down view) of the image, again specifying points\n",
        "\t# in the top-left, top-right, bottom-right, and bottom-left\n",
        "\t# order\n",
        "\tdst = np.array([\n",
        "\t\t[0, 0],\n",
        "\t\t[maxWidth - 1, 0],\n",
        "\t\t[maxWidth - 1, maxHeight - 1],\n",
        "\t\t[0, maxHeight - 1]], dtype = \"float32\")\n",
        "\t# compute the perspective transform matrix and then apply it\n",
        "\tM = cv2.getPerspectiveTransform(rect, dst)\n",
        "\twarped = cv2.warpPerspective(image, M, (maxWidth, maxHeight))\n",
        "\t# return the warped image\n",
        "\treturn warped"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-25T18:03:14.333614Z",
          "iopub.execute_input": "2022-03-25T18:03:14.333906Z",
          "iopub.status.idle": "2022-03-25T18:03:14.344658Z",
          "shell.execute_reply.started": "2022-03-25T18:03:14.333872Z",
          "shell.execute_reply": "2022-03-25T18:03:14.343934Z"
        },
        "trusted": true,
        "id": "9iMF5Da-RHg1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def contourApprox(imgOriginal, cnts):\n",
        "\t# initialize a contour that corresponds to the receipt outline\n",
        "\treceiptCnt = []\n",
        "\tfirstContour = False\n",
        "\tfirstArea = 0\n",
        "\tfirstPeri = 0\n",
        "\n",
        "\t# loop over the contours\n",
        "\tfor c in cnts:\n",
        "\t\t# approximate the contour\n",
        "\t\tperi = cv2.arcLength(c, True) + 0.01\n",
        "\t\tapprox = cv2.approxPolyDP(c, 0.05 * peri, True)\n",
        "\n",
        "\t\tarea = cv2.contourArea(approx) + 0.01\n",
        "\n",
        "\t\tdrawnContour = imgOriginal.copy()\n",
        "\t\tcv2.drawContours(drawnContour, [approx], -1, (0, 255, 0), 2)\n",
        "# \t\tplt.imshow(drawnContour)\n",
        "# \t\tplt.show()\n",
        "\t\t\n",
        "\t\tif area < 3000:\n",
        "\t\t\tbreak\n",
        "\n",
        "\t\t# if our approximated contour has four points, then we can\n",
        "\t\t# assume we have found the outline of the receipt\n",
        "\t\tif len(approx) == 4 and (not firstContour or (firstPeri / peri < 1.4 and firstPeri / peri > 0.7 and firstArea / area < 1.4 and firstArea / area > 0.7)):\n",
        "\t\t\treceiptCnt.append(approx)\n",
        "\t\t\tif firstContour:\n",
        "\t\t\t\t break\n",
        "\t\t\tfirstContour = True\n",
        "\t\t\tfirstArea = area\n",
        "\t\t\tfirstPeri = peri\n",
        "\t\t\t\n",
        "\t\telse:\n",
        "\t\t\tapprox = cv2.convexHull(approx)\n",
        "\t\t\tperi = cv2.arcLength(approx, True) + 0.01\n",
        "\t\t\tarea = cv2.contourArea(approx) + 0.01\n",
        "\t\t\tif len(approx) == 4 and (not firstContour or (firstPeri / peri < 1.4 and firstPeri / peri > 0.7 and firstArea / area < 1.4 and firstArea / area > 0.7)):\n",
        "\t\t\t\treceiptCnt.append(approx)\n",
        "\t\t\t\tif firstContour:\n",
        "\t\t\t\t\tbreak\n",
        "\t\t\t\tfirstContour = True\n",
        "\t\t\t\tfirstArea = area\n",
        "\t\t\t\tfirstPeri = peri\n",
        "\t\t\t\tbreak\t\t\n",
        "\t# if the receipt contour is empty then our script could not find the\n",
        "\t# outline and we should be notified\n",
        "\treturn receiptCnt"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-25T18:03:15.818789Z",
          "iopub.execute_input": "2022-03-25T18:03:15.819640Z",
          "iopub.status.idle": "2022-03-25T18:03:15.829898Z",
          "shell.execute_reply.started": "2022-03-25T18:03:15.819595Z",
          "shell.execute_reply": "2022-03-25T18:03:15.829152Z"
        },
        "trusted": true,
        "id": "K10MHZ7LRHg2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def showApprox(imgOriginal, thresh, receiptCnt):\n",
        "    cv2.drawContours(imgOriginal, [receiptCnt], -1, (0, 255, 0), 2)\n",
        "    #print(output[1])\n",
        "    #plt.imshow(imgOriginal)\n",
        "    #plt.show()\n",
        "    rect_coor = np.array(receiptCnt.reshape(4,2))\n",
        "    img = four_point_transform(imgOriginal, rect_coor)\n",
        "    #print(img.shape)\n",
        "    #img_new = add_margin(img, 5, 10, 0, 10)\n",
        "    final_img = preprocess2(img)\n",
        "#     plt.imshow(final_img, cmap=\"gray\")\n",
        "#     plt.show()\n",
        "    return (final_img,img)\n",
        "    #print(f\"boundaries = {receiptCnt.reshape(4, 2)}\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-25T18:03:17.751738Z",
          "iopub.execute_input": "2022-03-25T18:03:17.752020Z",
          "iopub.status.idle": "2022-03-25T18:03:17.758224Z",
          "shell.execute_reply.started": "2022-03-25T18:03:17.751988Z",
          "shell.execute_reply": "2022-03-25T18:03:17.757471Z"
        },
        "trusted": true,
        "id": "BjoeITDQRHg3"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def CutLetters(img,rgb_img, count):\n",
        "    height, width = img.shape\n",
        "    windowWidthR = int(0.18 * width * count)\n",
        "    windowWidthL = int(0.12 * width * count)\n",
        "    windowWidth = windowWidthR\n",
        "    if count == 2:\n",
        "        windowWidthR = windowWidthL\n",
        "    startRatio = int(0.1 * windowWidth)\n",
        "    windowHeight = img.shape[0]\n",
        "    marginWidth = int(windowWidth * 0.05)\n",
        "    stepSize = 8\n",
        "    GroupSize = 8\n",
        "    lowerBlackLimit = 0.07\n",
        "    upperBlackLimit = 0.9\n",
        "\n",
        "    Letters = []\n",
        "    Letters_imgs = []\n",
        "    Letters_imgs_thresh = []\n",
        "    for i in range(0, width - (windowWidth), stepSize * GroupSize):\n",
        "\n",
        "        Group = []\n",
        "        for j in range(i, min(i + stepSize * GroupSize, width - (windowWidth)), stepSize):\n",
        "            #plt.imshow(img[:, j:j+windowWidth],cmap='gray')\n",
        "            #plt.show()\n",
        "            if j <= width/2:\n",
        "                windowWidth = windowWidthL\n",
        "            else:\n",
        "                windowWidth = windowWidthR\n",
        "\n",
        "            blackCountMarginL = marginWidth * height  - np.count_nonzero(img[:, j : j+marginWidth])\n",
        "            blackCountMarginR = marginWidth * height  - np.count_nonzero(img[:, j+windowWidth-marginWidth : j+windowWidth])\n",
        "            blackCountInner = (windowWidth - 2 * startRatio) * height - np.count_nonzero(img[:, j+startRatio : j+windowWidth-startRatio])\n",
        "            \n",
        "            #print(blackCountMarginL, marginWidth * height)\n",
        "            #print(blackCountMarginR, marginWidth * height)\n",
        "            #print(blackCountInner, (windowWidth - 2 * marginWidth) * height)\n",
        "            #and blackCountMarginR < 0.2 * marginWidth * height \\\n",
        "\n",
        "            if blackCountMarginL < 0.25 * marginWidth * height and blackCountMarginR < 0.17 * marginWidth * height and blackCountInner > lowerBlackLimit * (windowWidth - 2 * startRatio) * height \\\n",
        "                and blackCountInner < upperBlackLimit * (windowWidth - 2 * startRatio) * height:\n",
        "                Group.append((blackCountInner, j))\n",
        "        if len(Group) > 0:\n",
        "            max_G = max(Group)[1]\n",
        "            if len(Letters) == 0 or len(Letters) > 0 and max_G - Letters[-1] > 21:\n",
        "                Letters.append(max_G)\n",
        "              # rect_coord = [rgb_img[0][0],rgb_img[0][-1],max(Group)[1],max(Group)[1]+windowWidth]\n",
        "              # rect_coord = np.array(rect_coord,dtype='int32')\n",
        "                rgb2_img = rgb_img[:, max_G:max_G+windowWidth]\n",
        "                Letters_imgs.append((rgb2_img, max_G))\n",
        "                #               plt.imshow(rgb2_img)\n",
        "                thresh_img = img[:, max_G:max_G+windowWidth]\n",
        "                Letters_imgs_thresh.append((thresh_img, max_G))\n",
        "              #plt.imshow(img[:, max(Group)[1]:max(Group)[1]+windowWidth],cmap='gray')\n",
        "              #plt.show()\n",
        "\n",
        "    return Letters_imgs,Letters_imgs_thresh\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-25T18:03:18.847371Z",
          "iopub.execute_input": "2022-03-25T18:03:18.849874Z",
          "iopub.status.idle": "2022-03-25T18:03:18.870832Z",
          "shell.execute_reply.started": "2022-03-25T18:03:18.849828Z",
          "shell.execute_reply": "2022-03-25T18:03:18.870170Z"
        },
        "trusted": true,
        "id": "NK3SkHtIRHg3"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sideBorder(img, d):\n",
        "    h,w=img.shape[0:2]\n",
        "    base_size=h+20,w+20,3\n",
        "    if d == 1:\n",
        "        base_size=h+20,w+20\n",
        "    # make a 3 channel image for base which is slightly larger than target img\n",
        "    base=np.zeros(base_size,dtype=np.uint8)\n",
        "    if d == 1:\n",
        "        cv2.rectangle(base,(0,0),(w+20,h+20),255,20) # really thick white rectangle\n",
        "    else:\n",
        "        cv2.rectangle(base,(0,0),(w+20,h+20),(255,255,255),20) # really thick white rectangle\n",
        "    \n",
        "    base[10:h+10,10:w+10]=img # this works\n",
        "    return base"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-25T18:03:21.045525Z",
          "iopub.execute_input": "2022-03-25T18:03:21.045834Z",
          "iopub.status.idle": "2022-03-25T18:03:21.054007Z",
          "shell.execute_reply.started": "2022-03-25T18:03:21.045799Z",
          "shell.execute_reply": "2022-03-25T18:03:21.053198Z"
        },
        "trusted": true,
        "id": "TDtyQDgaRHg4"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ImagetoSymbols(imgOriginal):    \n",
        "    #imgOriginal = cv2.imread(\"./train_images/02937.jpg\")\n",
        "    #img = imgOriginal.copy()\n",
        "    imgOriginal = imutils.resize(imgOriginal, width=500)\n",
        "    #ratio = img.shape[1] / float(imgOriginal.shape[1])\n",
        "#     plt.imshow(imgOriginal)\n",
        "#     plt.show()\n",
        "    gray,thresh = preprocess(imgOriginal)\n",
        "    # plt.imshow(gray,cmap='gray')\n",
        "    # plt.show()\n",
        "#     plt.imshow(thresh,cmap='gray')\n",
        "#     plt.show()\n",
        "\n",
        "    #ClosedThresh = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, cv2.getStructuringElement(cv2.MORPH_RECT, (15, 3)))\n",
        "    OpenedThresh = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, cv2.getStructuringElement(cv2.MORPH_RECT, (7, 7)))\n",
        "\n",
        "    minValue = np.min(OpenedThresh)\n",
        "\n",
        "    OpenedThresh[imgOriginal.shape[0]-4:imgOriginal.shape[0]-1, :] = minValue\n",
        "    OpenedThresh[:, imgOriginal.shape[1]-4:imgOriginal.shape[1]-1] = minValue\n",
        "    OpenedThresh[:, 0:3] = minValue\n",
        "\n",
        "\n",
        "#     plt.imshow(OpenedThresh,cmap='gray')\n",
        "#     plt.show()\n",
        "\n",
        "\n",
        "\n",
        "    #plt.imshow(ClosedThresh,cmap='gray')\n",
        "    #plt.show()\n",
        "    \n",
        "\n",
        "    cnts = cv2.findContours(OpenedThresh.copy(), cv2.RETR_EXTERNAL,\n",
        "        cv2.CHAIN_APPROX_SIMPLE)\n",
        "    cnts = imutils.grab_contours(cnts)\n",
        "    cnts = sorted(cnts, key=cv2.contourArea, reverse=True)\n",
        "    # for c in cnts:\n",
        "        # #print(c[0])\n",
        "        # plt.scatter(c[:, 0, 0], -c[:, 0, 1])\n",
        "        # plt.show()\n",
        "    # np.array(cnts,dtype=np.int32).shape\n",
        "    #print(cnts[-1])\n",
        "    receiptCnt = contourApprox(imgOriginal, cnts)\n",
        "    if len(receiptCnt) == 0:\n",
        "        \n",
        "        receiptCnt = [np.array([[[0,  0]], [[0, imgOriginal.shape[0]]],\n",
        "         [[imgOriginal.shape[1], imgOriginal.shape[0]]], [[imgOriginal.shape[1], 0]]])]\n",
        "    #print(receiptCnt)\n",
        "\n",
        "    numbers_cropped = []\n",
        "    numbers_cropped_thresh = []\n",
        "    letters_cropped = []\n",
        "    letters_cropped_thresh = []\n",
        "\n",
        "\n",
        "    if len(receiptCnt) == 2:\n",
        "        x = 1\n",
        "        avg1 = np.average(receiptCnt[0][:,0,0])\n",
        "        avg2 = np.average(receiptCnt[1][:,0,0])\n",
        "        if avg2 < avg1:\n",
        "            receiptCnt[0],receiptCnt[1] = receiptCnt[1],receiptCnt[0] \n",
        "\n",
        "        final_img1 ,rgb_img1 = showApprox(imgOriginal, thresh, receiptCnt[0])\n",
        "        final_img1 = sideBorder(final_img1, 1)\n",
        "        rgb_img1 = sideBorder(rgb_img1, 3)\n",
        "\n",
        "\n",
        "\n",
        "        symbols_cropped, symbols_cropped_thresh = CutLetters(final_img1,rgb_img1, 2)\n",
        "        for i in range(len(symbols_cropped)):\n",
        "            numbers_cropped.append(symbols_cropped[i][0])\n",
        "            numbers_cropped_thresh.append(symbols_cropped_thresh[i][0])\n",
        "\n",
        "        final_img1 ,rgb_img1 = showApprox(imgOriginal, thresh, receiptCnt[1])\n",
        "        final_img1 = sideBorder(final_img1, 1)\n",
        "        rgb_img1 = sideBorder(rgb_img1, 3)\n",
        "\n",
        "\n",
        "\n",
        "        symbols_cropped, symbols_cropped_thresh = CutLetters(final_img1,rgb_img1, 2)\n",
        "        for i in range(len(symbols_cropped)):\n",
        "            letters_cropped.append(symbols_cropped[i][0])\n",
        "            letters_cropped_thresh.append(symbols_cropped_thresh[i][0])\n",
        "\n",
        "    \n",
        "    else:\n",
        "        avg = np.average(receiptCnt[0][:,0,0])\n",
        "        final_img1 ,rgb_img1 = showApprox(imgOriginal, thresh, receiptCnt[0])\n",
        "        final_img1 = sideBorder(final_img1, 1)\n",
        "        rgb_img1 = sideBorder(rgb_img1, 3)\n",
        "\n",
        "        if final_img1.shape[1] < 280:\n",
        "            if avg < final_img1.shape[1] / 2:\n",
        "                symbols_cropped, symbols_cropped_thresh = CutLetters(final_img1,rgb_img1, 2)\n",
        "                for i in range(len(symbols_cropped)):\n",
        "                    numbers_cropped.append(symbols_cropped[i][0])\n",
        "                    numbers_cropped_thresh.append(symbols_cropped_thresh[i][0])\n",
        "            else:\n",
        "                symbols_cropped, symbols_cropped_thresh = CutLetters(final_img1,rgb_img1, 2)\n",
        "                for i in range(len(symbols_cropped)):\n",
        "                    letters_cropped.append(symbols_cropped[i][0])\n",
        "                    letters_cropped_thresh.append(symbols_cropped_thresh[i][0])\n",
        "        else: \n",
        "            symbols_cropped, symbols_cropped_thresh = CutLetters(final_img1,rgb_img1, 1)\n",
        "            for i in range(len(symbols_cropped)):\n",
        "                if symbols_cropped[i][1] < final_img1.shape[1] / 2:\n",
        "                    numbers_cropped.append(symbols_cropped[i][0])\n",
        "                    numbers_cropped_thresh.append(symbols_cropped_thresh[i][0])\n",
        "                else:\n",
        "                    letters_cropped.append(symbols_cropped[i][0])\n",
        "                    letters_cropped_thresh.append(symbols_cropped_thresh[i][0])\n",
        "    return numbers_cropped_thresh, letters_cropped_thresh\n",
        "    #print(numbers_cropped_thresh)\n",
        "    #for r in range(receiptCnt):\n",
        "\n",
        "\n",
        "    #    final_img ,rgb_img= showApprox(imgOriginal, thresh, r)\n",
        "    #    final_img = sideBorder(final_img, 1)\n",
        "    #    rgb_img = sideBorder(rgb_img, 3)\n",
        "\n",
        "\n",
        "    #    letters_cropped, letters_cropped_thresh = CutLetters(final_img,rgb_img, len(receiptCnt))\n",
        "    #    for character in letters_cropped_thresh:\n",
        "    #        charcater = np.array(character)\n",
        "    #        plt.imshow(charcater)\n",
        "    #        plt.show()\n",
        "    #        #print(predict(img=character, model=model, labels=labels))\n",
        "    #        print(predict(character))\n",
        "    #      #print(final_img.shape[1])\n",
        "    #      #CharacterContours(final_img)\n",
        "\n",
        "# plt.plot(cnts[0])\n",
        "# plt.show()\n",
        "\n",
        "#cnts = skimage.measure.find_contours(thresh)\n",
        "#cnts = sorted(cnts, reverse=True)\n",
        "#cnts\n",
        "\n",
        "# for c in cnts:\n",
        "#     # compute the bounding box of the contour and then use\n",
        "#     # the bounding box to derive the aspect ratio\n",
        "#     (x, y, w, h) = cv2.boundingRect(c)\n",
        "#     ar = w / float(h)\n",
        "#     if ar >= 2 and ar <= 10:\n",
        "#         lpCnt = c\n",
        "#         letter = gray[y:y + h, x:x + w]\n",
        "#         roi = cv2.threshold(letter, 0, 255,\n",
        "#                 cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]\n",
        "#             # check to see if we should clear any foreground\n",
        "#             # pixels touching the border of the image\n",
        "#             # (which typically, not but always, indicates noise)\n",
        "#         # display any debugging information and then break\n",
        "#         # from the loop early since we have found the license\n",
        "#         # plate region\n",
        "#         # plt.plot(letter,cmap='gray')\n",
        "#         # plt.show()\n",
        "#         plt.plot(roi,cmap='gray')\n",
        "#         plt.show()\n",
        "#         print(\"****************\")\n",
        "# print(roi)   "
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-25T18:03:22.142806Z",
          "iopub.execute_input": "2022-03-25T18:03:22.143242Z",
          "iopub.status.idle": "2022-03-25T18:03:22.166167Z",
          "shell.execute_reply.started": "2022-03-25T18:03:22.143201Z",
          "shell.execute_reply": "2022-03-25T18:03:22.165240Z"
        },
        "trusted": true,
        "id": "AWKLLM9uRHg5"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array, array_to_img\n",
        "\n",
        "def change_size(image):\n",
        "    img = array_to_img(image, scale=False) #returns PIL Image\n",
        "    img = img.resize((32, 32)) #resize image\n",
        "    arr = img_to_array(img) #convert back to array\n",
        "    return arr.astype(np.float64)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-25T18:03:26.722635Z",
          "iopub.execute_input": "2022-03-25T18:03:26.722924Z",
          "iopub.status.idle": "2022-03-25T18:03:26.728381Z",
          "shell.execute_reply.started": "2022-03-25T18:03:26.722893Z",
          "shell.execute_reply": "2022-03-25T18:03:26.727532Z"
        },
        "trusted": true,
        "id": "mQL2k9wfRHg7"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainData = []\n",
        "trainLabels = []\n",
        "\n",
        "letter_directory=\"./cropped_letters/cropped_letters/*/*\"\n",
        "number_directory=\"./cropped_numbers/cropped_numbers/*/*\"\n",
        "paths = glob.glob(letter_directory)\n",
        "paths = paths + glob.glob(number_directory)\n",
        "\n",
        "for path in paths:\n",
        "    img = io.imread(path)\n",
        "    imgSaturation, imgGrayscale = extractValue(img)\n",
        "    #imgMaxContrastGrayscale = maximizeContrast(imgGrayscale)\n",
        "    #imgBlurred = cv2.GaussianBlur(imgGrayscale, GAUSSIAN_SMOOTH_FILTER_SIZE, 0)\n",
        "    #imgThreshValue = cv2.threshold(imgBlurred, 0, 255, cv2.THRESH_OTSU)[1]\n",
        "#     imgPadded = gray2rgb(imgThreshValue)\n",
        "    imgPadded = resize(img,(32,32,1) )\n",
        "    trainData.append(imgPadded)\n",
        "    img_label = path.split('/')[-2]\n",
        "    trainLabels.append(img_label)\n",
        "\n",
        "trainData = np.array(trainData)\n",
        "trainLabels = np.array(trainLabels)\n",
        "\n",
        "print(trainLabels.shape)\n",
        "print(trainData.shape)\n",
        "print(trainData[0].shape)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-25T18:03:29.882754Z",
          "iopub.execute_input": "2022-03-25T18:03:29.883247Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-BGfL-KRHg8",
        "outputId": "f8f31708-07a9-4f8c-b8c2-f98b020a6c69"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(25002,)\n",
            "(25002, 32, 32, 1)\n",
            "(32, 32, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Apply augmentation\n",
        "- rotation, shifts, brightness change"
      ],
      "metadata": {
        "id": "PiGxhROoRHg8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The flow_from_directory() method allows you to read the images directly from the directory and augment them while the neural network model is learning on the training data.\n",
        "train_datagen = ImageDataGenerator(rescale=1./255,)\n",
        "#     shear_range=0.1,\n",
        "#     width_shift_range=0.1,\n",
        "#     height_shift_range=0.1,\n",
        "# #     zoom_range=0.2,\n",
        "#     rotation_range=20,\n",
        "    # validation_split=0.2) # set validation split\n",
        "\n",
        "# train_generator = train_datagen.flow(\n",
        "#     x=trainData, \n",
        "#     y=trainLabels,\n",
        "#     batch_size=128) # set as training data\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        './cropped_letters/cropped_letters',  # This is the source directory for training images\n",
        "        target_size=(32, 32),\n",
        "        batch_size=128,\n",
        "        class_mode='categorical')\n",
        "\n",
        "#     validation_generator = train_datagen.flow(\n",
        "#     x=trainData, \n",
        "#     y=trainLabels,\n",
        "#     batch_size=20,\n",
        "#     subset='validation') # set as validation data"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIYZHuvzRHg9",
        "outputId": "8e6fc409-8c49-476e-a7ea-7b1b38a1a3a5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 12509 images belonging to 28 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We split the data into the train and validation sets. Here is the distribution of the split data."
      ],
      "metadata": {
        "id": "c15MC8-RRHg9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# img, label = train_generator.next()\n",
        "# randindx = random.randint(0,128)\n",
        "# plt.imshow(img[randindx])"
      ],
      "metadata": {
        "trusted": true,
        "id": "ONU3jc7XRHg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "letter_model = Sequential([\n",
        "    tf.keras.layers.Conv2D(16, (22,22), input_shape=(32, 32, 3), activation='relu', padding='same'),\n",
        "    tf.keras.layers.Conv2D(16, (22,22), activation='relu', padding='same'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.MaxPooling2D((2,2)),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Conv2D(32, (16,16), activation='relu', padding='same'),\n",
        "    tf.keras.layers.Conv2D(32, (16,16), activation='relu', padding='same'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.MaxPooling2D((2,2)),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Conv2D(64, (8,8), activation='relu', padding='same'),\n",
        "    tf.keras.layers.Conv2D(64, (4,4), activation='relu', padding='same'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.MaxPooling2D((2,2)),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Conv2D(128, (4,4), activation='relu', padding='same'),\n",
        "    tf.keras.layers.Conv2D(128, (4,4), activation='relu', padding='same'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    \n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(28, activation='softmax')\n",
        "])\n",
        "\n",
        "letter_model.compile(optimizer=keras.optimizers.Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "history = letter_model.fit(train_generator, epochs=18, \n",
        "            steps_per_epoch=train_generator.n//train_generator.batch_size)"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzKnMTEfRHg9",
        "outputId": "74f39ecb-483e-430e-b328-5198b393dd4d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/18\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "97/97 [==============================] - 12s 78ms/step - loss: 2.7076 - accuracy: 0.1838\n",
            "Epoch 2/18\n",
            "97/97 [==============================] - 9s 89ms/step - loss: 1.0012 - accuracy: 0.6324\n",
            "Epoch 3/18\n",
            "97/97 [==============================] - 8s 86ms/step - loss: 0.5509 - accuracy: 0.7973\n",
            "Epoch 4/18\n",
            "97/97 [==============================] - 5s 49ms/step - loss: 0.3770 - accuracy: 0.8671\n",
            "Epoch 5/18\n",
            "97/97 [==============================] - 5s 48ms/step - loss: 0.2955 - accuracy: 0.8956\n",
            "Epoch 6/18\n",
            "97/97 [==============================] - 5s 48ms/step - loss: 0.2504 - accuracy: 0.9146\n",
            "Epoch 7/18\n",
            "97/97 [==============================] - 5s 48ms/step - loss: 0.2108 - accuracy: 0.9257\n",
            "Epoch 8/18\n",
            "97/97 [==============================] - 5s 50ms/step - loss: 0.1761 - accuracy: 0.9383\n",
            "Epoch 9/18\n",
            "97/97 [==============================] - 5s 49ms/step - loss: 0.1486 - accuracy: 0.9480\n",
            "Epoch 10/18\n",
            "97/97 [==============================] - 5s 50ms/step - loss: 0.1395 - accuracy: 0.9521\n",
            "Epoch 11/18\n",
            "97/97 [==============================] - 5s 48ms/step - loss: 0.1182 - accuracy: 0.9578\n",
            "Epoch 12/18\n",
            "97/97 [==============================] - 5s 48ms/step - loss: 0.1095 - accuracy: 0.9607\n",
            "Epoch 13/18\n",
            "97/97 [==============================] - 5s 51ms/step - loss: 0.0963 - accuracy: 0.9665\n",
            "Epoch 14/18\n",
            "97/97 [==============================] - 5s 49ms/step - loss: 0.0863 - accuracy: 0.9696\n",
            "Epoch 15/18\n",
            "97/97 [==============================] - 5s 49ms/step - loss: 0.0847 - accuracy: 0.9711\n",
            "Epoch 16/18\n",
            "97/97 [==============================] - 5s 49ms/step - loss: 0.0834 - accuracy: 0.9706\n",
            "Epoch 17/18\n",
            "97/97 [==============================] - 5s 48ms/step - loss: 0.0686 - accuracy: 0.9777\n",
            "Epoch 18/18\n",
            "97/97 [==============================] - 5s 49ms/step - loss: 0.0596 - accuracy: 0.9790\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        \n",
        "        if(logs.get('accuracy') > 0.98):\n",
        "            # Stop if threshold is met\n",
        "            print(\"\\naccuracy of training is bigger than 98!\")\n",
        "            self.model.stop_training = True\n",
        "\n",
        "# Instantiate class\n",
        "callbacks = myCallback()"
      ],
      "metadata": {
        "trusted": true,
        "id": "KBRvQ4gpRHg-"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ZLNiiycnkYxq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator2 = train_datagen.flow_from_directory(\n",
        "        './cropped_numbers/cropped_numbers',  # This is the source directory for training images\n",
        "        target_size=(32, 32),  \n",
        "        batch_size=128,\n",
        "        class_mode='categorical')"
      ],
      "metadata": {
        "id": "yTsejLIIojOK",
        "outputId": "e33f464b-8f72-44c9-d5d5-d7b8aaadde4d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 12493 images belonging to 10 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "number_model = Sequential([\n",
        "  tf.keras.layers.Conv2D(16, (22,22), input_shape=(32, 32, 3), activation='relu', padding='same'),\n",
        "  tf.keras.layers.Conv2D(16, (22,22), activation='relu', padding='same'),\n",
        "  tf.keras.layers.BatchNormalization(),\n",
        "  tf.keras.layers.MaxPooling2D((2,2)),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Conv2D(32, (16,16), activation='relu', padding='same'),\n",
        "  tf.keras.layers.Conv2D(32, (16,16), activation='relu', padding='same'),\n",
        "  tf.keras.layers.BatchNormalization(),\n",
        "  tf.keras.layers.MaxPooling2D((2,2)),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Conv2D(64, (8,8), activation='relu', padding='same'),\n",
        "  tf.keras.layers.Conv2D(64, (4,4), activation='relu', padding='same'),\n",
        "  tf.keras.layers.BatchNormalization(),\n",
        "  tf.keras.layers.MaxPooling2D((2,2)),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Conv2D(128, (4,4), activation='relu', padding='same'),\n",
        "  tf.keras.layers.Conv2D(128, (4,4), activation='relu', padding='same'),\n",
        "  tf.keras.layers.BatchNormalization(),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "number_model.compile(optimizer=keras.optimizers.Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "history = number_model.fit(train_generator2, epochs=8, callbacks=callbacks)"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFXNs0JoRHhA",
        "outputId": "7092acb8-aa27-4e81-8f76-ac27f0406239"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "98/98 [==============================] - 7s 59ms/step - loss: 2.1499 - accuracy: 0.2081\n",
            "Epoch 2/8\n",
            "98/98 [==============================] - 5s 56ms/step - loss: 0.5876 - accuracy: 0.8061\n",
            "Epoch 3/8\n",
            "98/98 [==============================] - 5s 48ms/step - loss: 0.1795 - accuracy: 0.9485\n",
            "Epoch 4/8\n",
            "98/98 [==============================] - 5s 49ms/step - loss: 0.0918 - accuracy: 0.9743\n",
            "Epoch 5/8\n",
            "97/98 [============================>.] - ETA: 0s - loss: 0.0696 - accuracy: 0.9803\n",
            "accuracy of training is bigger than 98!\n",
            "98/98 [==============================] - 5s 50ms/step - loss: 0.0697 - accuracy: 0.9802\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "number_labels = {v: k for k, v in train_generator2.class_indices.items()}\n",
        "letter_labels = {v: k for k, v in train_generator.class_indices.items()}\n",
        "print(number_labels)\n",
        "print(letter_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1uTkCPK0vxC",
        "outputId": "fa520f29-32e6-4502-817b-ff93e105c153"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: '0', 1: '1', 2: '2', 3: '3', 4: '4', 5: '5', 6: '6', 7: '7', 8: '8', 9: '9'}\n",
            "{0: 'ا', 1: 'ب', 2: 'ت', 3: 'ث', 4: 'ج', 5: 'ح', 6: 'خ', 7: 'د', 8: 'ذ', 9: 'ر', 10: 'ز', 11: 'س', 12: 'ش', 13: 'ص', 14: 'ض', 15: 'ط', 16: 'ظ', 17: 'ع', 18: 'غ', 19: 'ف', 20: 'ق', 21: 'ك', 22: 'ل', 23: 'م', 24: 'ن', 25: 'ه', 26: 'و', 27: 'ى'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(img, model, labels):\n",
        "    img = resize(img, (32,32,3), anti_aliasing=True)\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "\n",
        "    preds = model.predict(img)\n",
        "    return labels[np.argmax(preds)]"
      ],
      "metadata": {
        "trusted": true,
        "id": "zuSv7rhoRHhA"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "rows = []\n",
        "for i in range(691):\n",
        "    imgOriginal = io.imread(f\"./test_images/test_images/{str(i).zfill(5)}.jpg\") # to be replaced by image readerfrom loop\n",
        "    Nums, Chars = ImagetoSymbols(imgOriginal)\n",
        "\n",
        "    labelList = []\n",
        "\n",
        "    for char in Chars:\n",
        "      # predicting with letters' model\n",
        "      predicted_label = predict(char, model=letter_model, labels=letter_labels)\n",
        "      labelList.append(predicted_label)  \n",
        "      # plt.imshow(np.array(char))\n",
        "      # plt.show()\n",
        "      # print(predicted_label)\n",
        "\n",
        "    for num in Nums:\n",
        "      # predicting with numbers' model\n",
        "      predicted_label = predict(num, model=number_model, labels=number_labels)\n",
        "      labelList.append(predicted_label) \n",
        "      # plt.imshow(np.array(num))\n",
        "      # plt.show()\n",
        "      # print(predicted_label)\n",
        "      \n",
        "    label = ''.join(str(e) for e in labelList) \n",
        "\n",
        "    # csv header\n",
        "    headerNames = ['img_name', 'label']\n",
        "\n",
        "    # csv data\n",
        "    imageID = f'{str(i).zfill(5)}.jpg' #replace with img id from loop\n",
        "    rows.append( \n",
        "        {'img_name': imageID,\n",
        "        'label': label})\n",
        "\n",
        "with open('./submissions.csv', 'w', encoding='UTF16', newline='') as f:\n",
        "    writer = csv.DictWriter(f, fieldnames=headerNames)\n",
        "    writer.writeheader()\n",
        "    writer.writerows(rows)"
      ],
      "metadata": {
        "trusted": true,
        "id": "HdFvBlB9RHhA"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"./submissions.csv\", encoding='UTF16')\n",
        "df.columns"
      ],
      "metadata": {
        "trusted": true,
        "id": "7XfXGkMaRHhA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e896b790-6aa4-4422-fb32-cea94c576766"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['img_name', 'label'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    }
  ]
}