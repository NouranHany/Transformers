{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Overview\n* Trained a model using the cropped images obtained from script crop right noise\n* Increasing the size of data by doing augmentation is not yet implemented","metadata":{}},{"cell_type":"markdown","source":"# Import Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n#Deep Learning\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers as L\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom skimage.transform import resize\n\nimport os\nimport cv2\nimport glob\nimport skimage.io as io\nimport pickle\nfrom skimage.color import rgb2gray\n\n#Data Visualizations\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2022-03-25T10:55:09.824940Z","iopub.execute_input":"2022-03-25T10:55:09.825209Z","iopub.status.idle":"2022-03-25T10:55:09.873172Z","shell.execute_reply.started":"2022-03-25T10:55:09.825176Z","shell.execute_reply":"2022-03-25T10:55:09.872526Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"trainData = []\ntrainLabels = []\n\ndirectory=\"/kaggle/input/croppeddata/cropped_characters/*/*\"\n\npaths = glob.glob(directory)\ncount=0\nfor path in paths:\n    img = io.imread(path)\n    imgBlurred = cv2.GaussianBlur(img, GAUSSIAN_SMOOTH_FILTER_SIZE, 0)\n    imgThreshValue = cv2.threshold(imgBlurred, 0, 255, cv2.THRESH_OTSU)[1]\n    imgPadded = sideBorder(imgThreshValue, 1)\n    \n    trainData.append(imgPadded)\n    img_label = path.split('/')[-2]\n    trainLabels.append(img_label)\n\ntrainData = np.array(trainData)\ntrainLabels = np.array(trainLabels)\n\nprint(trainLabels.shape)\nprint(trainData.shape)\nprint(trainData[0].shape)","metadata":{"execution":{"iopub.status.busy":"2022-03-25T09:32:48.845541Z","iopub.execute_input":"2022-03-25T09:32:48.846242Z","iopub.status.idle":"2022-03-25T09:33:31.841812Z","shell.execute_reply.started":"2022-03-25T09:32:48.846208Z","shell.execute_reply":"2022-03-25T09:33:31.841005Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Padding\n\n\n\n\n# Blur\n\n# Thresholding\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import load_img, img_to_array, array_to_img\n\ndef change_size(image):\n    img = array_to_img(image, scale=False) #returns PIL Image\n    img = img.resize((150, 150)) #resize image\n    arr = img_to_array(img) #convert back to array\n    return arr.astype(np.float64)","metadata":{"execution":{"iopub.status.busy":"2022-03-25T09:33:31.843463Z","iopub.execute_input":"2022-03-25T09:33:31.844180Z","iopub.status.idle":"2022-03-25T09:33:31.850373Z","shell.execute_reply.started":"2022-03-25T09:33:31.844142Z","shell.execute_reply":"2022-03-25T09:33:31.849418Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Apply augmentation\n- rotation, shifts, brightness change","metadata":{}},{"cell_type":"code","source":"# The flow_from_directory() method allows you to read the images directly from the directory and augment them while the neural network model is learning on the training data.\ntrain_datagen = ImageDataGenerator(rescale=1./255,\n    shear_range=0.2,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n#     zoom_range=0.2,\n    rotation_range=30,\n    validation_split=0.2) # set validation split\n\ntrain_generator = train_datagen.flow_from_directory(\n    '/kaggle/input/croppeddata/cropped_characters',\n    target_size=(150, 150),\n    batch_size=128,\n    class_mode='categorical',\n    subset='training') # set as training data\n\nvalidation_generator = train_datagen.flow_from_directory(\n    '/kaggle/input/croppeddata/cropped_characters', # same directory as training data\n    target_size=(150, 150),\n    batch_size=20,\n    class_mode='categorical',\n    subset='validation') # set as validation data","metadata":{"execution":{"iopub.status.busy":"2022-03-25T11:12:07.899695Z","iopub.execute_input":"2022-03-25T11:12:07.900003Z","iopub.status.idle":"2022-03-25T11:12:10.774630Z","shell.execute_reply.started":"2022-03-25T11:12:07.899971Z","shell.execute_reply":"2022-03-25T11:12:10.773828Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"markdown","source":"We split the data into the train and validation sets. Here is the distribution of the split data.","metadata":{}},{"cell_type":"code","source":"sns.barplot(['train', 'valid'], [train_generator.n, validation_generator.n])","metadata":{"execution":{"iopub.status.busy":"2022-03-25T09:18:25.144470Z","iopub.execute_input":"2022-03-25T09:18:25.144778Z","iopub.status.idle":"2022-03-25T09:18:25.369518Z","shell.execute_reply.started":"2022-03-25T09:18:25.144737Z","shell.execute_reply":"2022-03-25T09:18:25.368631Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# model = Sequential()\n\n# model.add(tf.keras.applications.resnet50.ResNet50(input_shape = (150, 150, 3), \n#                                 include_top = False, \n#                                 weights = 'imagenet'))\n\n# model.add(L.Flatten())\n# model.add(tf.keras.layers.Dropout(0.2))\n# model.add(L.Dense(256, activation='relu'))\n# model.add(tf.keras.layers.Dropout(0.2))\n# model.add(L.Dense(128, activation='relu'))\n# model.add(L.Dense(38, activation='softmax'))\n\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(150, 150, 3)),\n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n    tf.keras.layers.MaxPooling2D(2,2), \n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Conv2D(64, (3,3), activation='relu'), \n    tf.keras.layers.MaxPooling2D(2,2),\n    \n    # Flatten the results to feed into a DNN\n    tf.keras.layers.Flatten(), \n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(256, activation='relu'),\n    tf.keras.layers.Dense(128, activation='relu'), \n    \n    tf.keras.layers.Dense(38, activation='softmax')  \n])\n\nmodel.compile(optimizer=keras.optimizers.Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n#Do not use default learning rate since it is too high!","metadata":{"execution":{"iopub.status.busy":"2022-03-25T11:15:01.276289Z","iopub.execute_input":"2022-03-25T11:15:01.277021Z","iopub.status.idle":"2022-03-25T11:15:01.358678Z","shell.execute_reply.started":"2022-03-25T11:15:01.276984Z","shell.execute_reply":"2022-03-25T11:15:01.357297Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"code","source":"class myCallback(tf.keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs={}):\n        # Check accuracy\n        if(logs.get('accuracy') > 90):\n            # Stop if threshold is met\n            print(\"\\naccuracy of training is bigger than 90!\")\n            self.model.stop_training = True\n\n# Instantiate class\ncallbacks = myCallback()","metadata":{"execution":{"iopub.status.busy":"2022-03-25T12:27:31.856130Z","iopub.execute_input":"2022-03-25T12:27:31.856399Z","iopub.status.idle":"2022-03-25T12:27:31.861403Z","shell.execute_reply.started":"2022-03-25T12:27:31.856371Z","shell.execute_reply":"2022-03-25T12:27:31.860485Z"},"trusted":true},"execution_count":93,"outputs":[]},{"cell_type":"code","source":"# for layer in model.layers[0].layers:\n#     if layer.name == 'conv5_block1_0_conv':\n#         break\n#     layer.trainable=False","metadata":{"execution":{"iopub.status.busy":"2022-03-25T11:15:09.432953Z","iopub.execute_input":"2022-03-25T11:15:09.433210Z","iopub.status.idle":"2022-03-25T11:15:09.436926Z","shell.execute_reply.started":"2022-03-25T11:15:09.433181Z","shell.execute_reply":"2022-03-25T11:15:09.435921Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"code","source":"history = model.fit(train_generator, epochs=20, \n            validation_data=validation_generator,\n            steps_per_epoch=train_generator.n//train_generator.batch_size,\n            validation_steps=validation_generator.n//validation_generator.batch_size)","metadata":{"execution":{"iopub.status.busy":"2022-03-25T11:15:27.420462Z","iopub.execute_input":"2022-03-25T11:15:27.421132Z","iopub.status.idle":"2022-03-25T12:08:13.851131Z","shell.execute_reply.started":"2022-03-25T11:15:27.421083Z","shell.execute_reply":"2022-03-25T12:08:13.850345Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"code","source":"#-----------------------------------------------------------\n# Retrieve a list of list results on training and test data\n# sets for each training epoch\n#-----------------------------------------------------------\nacc      = history.history[     'accuracy' ]\nval_acc  = history.history[ 'val_accuracy' ]\nloss     = history.history[    'loss' ]\nval_loss = history.history['val_loss' ]\n\nepochs   = range(len(acc)) # Get number of epochs\n\n#------------------------------------------------\n# Plot training and validation accuracy per epoch\n#------------------------------------------------\nplt.plot  ( epochs,     acc )\nplt.plot  ( epochs, val_acc )\nplt.title ('Training and validation accuracy')\nplt.legend(['train', 'test'])\nplt.figure()\n\n#------------------------------------------------\n# Plot training and validation loss per epoch\n#------------------------------------------------\nplt.plot  ( epochs,     loss )\nplt.plot  ( epochs, val_loss )\nplt.title ('Training and validation loss'   )\nplt.legend(['train', 'test'])","metadata":{"execution":{"iopub.status.busy":"2022-03-25T12:08:52.951555Z","iopub.execute_input":"2022-03-25T12:08:52.951819Z","iopub.status.idle":"2022-03-25T12:08:53.399759Z","shell.execute_reply.started":"2022-03-25T12:08:52.951792Z","shell.execute_reply":"2022-03-25T12:08:53.398291Z"},"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"code","source":"# save the model to disk\ndirpath = 'char_detect_model'\n\nmodel.save('char_detect_model')","metadata":{"execution":{"iopub.status.busy":"2022-03-25T12:13:43.061435Z","iopub.execute_input":"2022-03-25T12:13:43.062129Z","iopub.status.idle":"2022-03-25T12:13:44.883822Z","shell.execute_reply.started":"2022-03-25T12:13:43.062072Z","shell.execute_reply":"2022-03-25T12:13:44.883103Z"},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"code","source":"model = keras.models.load_model('char_detect_model')\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-03-25T12:30:27.429705Z","iopub.execute_input":"2022-03-25T12:30:27.429968Z","iopub.status.idle":"2022-03-25T12:30:28.002625Z","shell.execute_reply.started":"2022-03-25T12:30:27.429940Z","shell.execute_reply":"2022-03-25T12:30:28.001688Z"},"trusted":true},"execution_count":94,"outputs":[]},{"cell_type":"code","source":"history = model.fit(train_generator, epochs=40, \n            validation_data=validation_generator,\n            steps_per_epoch=train_generator.n//train_generator.batch_size,\n            validation_steps=validation_generator.n//validation_generator.batch_size, callbacks=[callbacks])","metadata":{"execution":{"iopub.status.busy":"2022-03-25T12:30:31.953434Z","iopub.execute_input":"2022-03-25T12:30:31.953940Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels = os.listdir('../input/croppeddata/cropped_characters')\n\ndef predict(img, model, labels):\n    img = resize(img, (150,150,3), anti_aliasing=True)\n    img = np.expand_dims(img, axis=0)\n    \n    preds = model.predict(img)\n    return labels[np.argmax(preds)]","metadata":{"execution":{"iopub.status.busy":"2022-03-25T10:56:36.937785Z","iopub.execute_input":"2022-03-25T10:56:36.938365Z","iopub.status.idle":"2022-03-25T10:56:36.946816Z","shell.execute_reply.started":"2022-03-25T10:56:36.938322Z","shell.execute_reply":"2022-03-25T10:56:36.946028Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"print(labels)","metadata":{"execution":{"iopub.status.busy":"2022-03-25T10:55:29.967441Z","iopub.execute_input":"2022-03-25T10:55:29.967698Z","iopub.status.idle":"2022-03-25T10:55:29.973602Z","shell.execute_reply.started":"2022-03-25T10:55:29.967670Z","shell.execute_reply":"2022-03-25T10:55:29.972140Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"#!pip install imutils","metadata":{"execution":{"iopub.status.busy":"2022-03-25T10:18:59.757800Z","iopub.execute_input":"2022-03-25T10:18:59.758569Z","iopub.status.idle":"2022-03-25T10:18:59.763791Z","shell.execute_reply.started":"2022-03-25T10:18:59.758533Z","shell.execute_reply":"2022-03-25T10:18:59.762902Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"#import pytesseract\nfrom imutils import paths\nimport argparse\nimport imutils\nimport cv2\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nfrom skimage.segmentation import clear_border\nimport numpy as np\nimport skimage \nfrom PIL import Image\n#from imutils.perspective import four_point_transform\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2022-03-25T10:59:04.387846Z","iopub.execute_input":"2022-03-25T10:59:04.388416Z","iopub.status.idle":"2022-03-25T10:59:04.396588Z","shell.execute_reply.started":"2022-03-25T10:59:04.388377Z","shell.execute_reply":"2022-03-25T10:59:04.395851Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"# module level variables ##########################################################################\nGAUSSIAN_SMOOTH_FILTER_SIZE = (5, 5)\nADAPTIVE_THRESH_BLOCK_SIZE = 19\nADAPTIVE_THRESH_WEIGHT = 9\n\n###################################################################################################\ndef preprocess(imgOriginal):\n    imgSaturation, imgGrayscale = extractValue(imgOriginal)\n\n    imgMaxContrastGrayscale = maximizeContrast(imgGrayscale)\n\n    imgBlurred = cv2.GaussianBlur(imgMaxContrastGrayscale, GAUSSIAN_SMOOTH_FILTER_SIZE, 0)\n\n    imgThreshValue = cv2.threshold(imgBlurred, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]\n    imgThreshSaturation = cv2.threshold(imgSaturation, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]\n\n    # plt.imshow(imgBlurred,cmap='gray')\n    # plt.show()\n    # plt.imshow(imgThreshValue,cmap='gray')\n    # plt.show()\n\n    # plt.imshow(imgSaturation,cmap='gray')\n    # plt.show()\n    # plt.imshow(imgThreshSaturation,cmap='gray')\n    # plt.show()\n\n    #imgThresh = cv2.adaptiveThreshold(imgBlurred, 255.0, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, ADAPTIVE_THRESH_BLOCK_SIZE, ADAPTIVE_THRESH_WEIGHT)\n    \n    return imgGrayscale, (imgThreshValue & (255 - imgThreshSaturation))\n# end function\n\ndef preprocess2(imgOriginal):\n    _, imgGrayscale = extractValue(imgOriginal)\n\n    imgBlurred = cv2.GaussianBlur(imgGrayscale, GAUSSIAN_SMOOTH_FILTER_SIZE, 0)\n\n    imgThreshValue = cv2.threshold(imgBlurred, 0, 255, cv2.THRESH_OTSU)[1]\n\n    # plt.imshow(imgBlurred,cmap='gray')\n    # plt.show()\n    # plt.imshow(imgThreshValue,cmap='gray')\n    # plt.show()\n\n    # plt.imshow(imgSaturation,cmap='gray')\n    # plt.show()\n    # plt.imshow(imgThreshSaturation,cmap='gray')\n    # plt.show()\n\n    #imgThresh = cv2.adaptiveThreshold(imgBlurred, 255.0, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, ADAPTIVE_THRESH_BLOCK_SIZE, ADAPTIVE_THRESH_WEIGHT)\n    \n    return imgThreshValue\n# end function\n\n###################################################################################################\ndef extractValue(imgOriginal):\n    height, width, numChannels = imgOriginal.shape\n\n    imgHSV = np.zeros((height, width, 3), np.uint8)\n\n    imgHSV = cv2.cvtColor(imgOriginal, cv2.COLOR_BGR2HSV)\n\n    imgHue, imgSaturation, imgValue = cv2.split(imgHSV)\n\n    return imgSaturation, imgValue\n# end function\n\n###################################################################################################\ndef maximizeContrast(imgGrayscale):\n\n    height, width = imgGrayscale.shape\n\n    imgTopHat = np.zeros((height, width, 1), np.uint8)\n    imgBlackHat = np.zeros((height, width, 1), np.uint8)\n\n    structuringElement = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n\n    imgTopHat = cv2.morphologyEx(imgGrayscale, cv2.MORPH_TOPHAT, structuringElement)\n    imgBlackHat = cv2.morphologyEx(imgGrayscale, cv2.MORPH_BLACKHAT, structuringElement)\n\n    imgGrayscalePlusTopHat = cv2.add(imgGrayscale, imgTopHat)\n    imgGrayscalePlusTopHatMinusBlackHat = cv2.subtract(imgGrayscalePlusTopHat, imgBlackHat)\n\n    return imgGrayscalePlusTopHatMinusBlackHat\n# end function","metadata":{"execution":{"iopub.status.busy":"2022-03-25T11:00:52.838038Z","iopub.execute_input":"2022-03-25T11:00:52.838605Z","iopub.status.idle":"2022-03-25T11:00:52.851388Z","shell.execute_reply.started":"2022-03-25T11:00:52.838567Z","shell.execute_reply":"2022-03-25T11:00:52.850580Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"def order_points(pts):\n\t# initialzie a list of coordinates that will be ordered\n\t# such that the first entry in the list is the top-left,\n\t# the second entry is the top-right, the third is the\n\t# bottom-right, and the fourth is the bottom-left\n\trect = np.zeros((4, 2), dtype = \"float32\")\n\t# the top-left point will have the smallest sum, whereas\n\t# the bottom-right point will have the largest sum\n\ts = pts.sum(axis = 1)\n\trect[0] = pts[np.argmin(s)]\n\trect[2] = pts[np.argmax(s)]\n\t# now, compute the difference between the points, the\n\t# top-right point will have the smallest difference,\n\t# whereas the bottom-left will have the largest difference\n\tdiff = np.diff(pts, axis = 1)\n\trect[1] = pts[np.argmin(diff)]\n\trect[3] = pts[np.argmax(diff)]\n\t# return the ordered coordinates\n\treturn rect","metadata":{"execution":{"iopub.status.busy":"2022-03-25T11:00:56.332460Z","iopub.execute_input":"2022-03-25T11:00:56.333111Z","iopub.status.idle":"2022-03-25T11:00:56.341001Z","shell.execute_reply.started":"2022-03-25T11:00:56.333073Z","shell.execute_reply":"2022-03-25T11:00:56.339040Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"def four_point_transform(image, pts):\n\t# obtain a consistent order of the points and unpack them\n\t# individually\n\trect = order_points(pts)\n\t(tl, tr, br, bl) = rect\n\t# compute the width of the new image, which will be the\n\t# maximum distance between bottom-right and bottom-left\n\t# x-coordiates or the top-right and top-left x-coordinates\n\twidthA = np.sqrt(((br[0] - bl[0]) ** 2) + ((br[1] - bl[1]) ** 2))\n\twidthB = np.sqrt(((tr[0] - tl[0]) ** 2) + ((tr[1] - tl[1]) ** 2))\n\tmaxWidth = max(int(widthA), int(widthB))\n\t# compute the height of the new image, which will be the\n\t# maximum distance between the top-right and bottom-right\n\t# y-coordinates or the top-left and bottom-left y-coordinates\n\theightA = np.sqrt(((tr[0] - br[0]) ** 2) + ((tr[1] - br[1]) ** 2))\n\theightB = np.sqrt(((tl[0] - bl[0]) ** 2) + ((tl[1] - bl[1]) ** 2))\n\tmaxHeight = max(int(heightA), int(heightB))\n\t# now that we have the dimensions of the new image, construct\n\t# the set of destination points to obtain a \"birds eye view\",\n\t# (i.e. top-down view) of the image, again specifying points\n\t# in the top-left, top-right, bottom-right, and bottom-left\n\t# order\n\tdst = np.array([\n\t\t[0, 0],\n\t\t[maxWidth - 1, 0],\n\t\t[maxWidth - 1, maxHeight - 1],\n\t\t[0, maxHeight - 1]], dtype = \"float32\")\n\t# compute the perspective transform matrix and then apply it\n\tM = cv2.getPerspectiveTransform(rect, dst)\n\twarped = cv2.warpPerspective(image, M, (maxWidth, maxHeight))\n\t# return the warped image\n\treturn warped","metadata":{"execution":{"iopub.status.busy":"2022-03-25T11:01:00.535661Z","iopub.execute_input":"2022-03-25T11:01:00.536251Z","iopub.status.idle":"2022-03-25T11:01:00.547040Z","shell.execute_reply.started":"2022-03-25T11:01:00.536206Z","shell.execute_reply":"2022-03-25T11:01:00.545914Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"def contourApprox(imgOriginal, cnts):\n\t# initialize a contour that corresponds to the receipt outline\n\treceiptCnt = []\n\tfirstContour = False\n\tfirstArea = 0\n\tfirstPeri = 0\n\n\t# loop over the contours\n\tfor c in cnts:\n\t\t# approximate the contour\n\t\tperi = cv2.arcLength(c, True) + 0.01\n\t\tapprox = cv2.approxPolyDP(c, 0.05 * peri, True)\n\n\t\tarea = cv2.contourArea(approx) + 0.01\n\n\t\tdrawnContour = imgOriginal.copy()\n\t\tcv2.drawContours(drawnContour, [approx], -1, (0, 255, 0), 2)\n# \t\tplt.imshow(drawnContour)\n# \t\tplt.show()\n\t\t\n\t\tif area < 3000:\n\t\t\tbreak\n\n\t\t# if our approximated contour has four points, then we can\n\t\t# assume we have found the outline of the receipt\n\t\tif len(approx) == 4 and (not firstContour or (firstPeri / peri < 1.4 and firstPeri / peri > 0.7 and firstArea / area < 1.4 and firstArea / area > 0.7)):\n\t\t\treceiptCnt.append(approx)\n\t\t\tif firstContour:\n\t\t\t\t break\n\t\t\tfirstContour = True\n\t\t\tfirstArea = area\n\t\t\tfirstPeri = peri\n\t\t\t\n\t\telse:\n\t\t\tapprox = cv2.convexHull(approx)\n\t\t\tperi = cv2.arcLength(approx, True) + 0.01\n\t\t\tarea = cv2.contourArea(approx) + 0.01\n\t\t\tif len(approx) == 4 and (not firstContour or (firstPeri / peri < 1.4 and firstPeri / peri > 0.7 and firstArea / area < 1.4 and firstArea / area > 0.7)):\n\t\t\t\treceiptCnt.append(approx)\n\t\t\t\tif firstContour:\n\t\t\t\t\tbreak\n\t\t\t\tfirstContour = True\n\t\t\t\tfirstArea = area\n\t\t\t\tfirstPeri = peri\n\t\t\t\tbreak\t\t\n\t# if the receipt contour is empty then our script could not find the\n\t# outline and we should be notified\n\treturn receiptCnt","metadata":{"execution":{"iopub.status.busy":"2022-03-25T11:01:02.247392Z","iopub.execute_input":"2022-03-25T11:01:02.247645Z","iopub.status.idle":"2022-03-25T11:01:02.258069Z","shell.execute_reply.started":"2022-03-25T11:01:02.247618Z","shell.execute_reply":"2022-03-25T11:01:02.256998Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"def showApprox(imgOriginal, thresh, receiptCnt):\n    cv2.drawContours(imgOriginal, [receiptCnt], -1, (0, 255, 0), 2)\n    #print(output[1])\n    plt.imshow(imgOriginal)\n    plt.show()\n    rect_coor = np.array(receiptCnt.reshape(4,2))\n    img = four_point_transform(imgOriginal, rect_coor)\n    #print(img.shape)\n    #img_new = add_margin(img, 5, 10, 0, 10)\n    final_img = preprocess2(img)\n#     plt.imshow(final_img, cmap=\"gray\")\n#     plt.show()\n    return (final_img,img)\n    #print(f\"boundaries = {receiptCnt.reshape(4, 2)}\")\n","metadata":{"execution":{"iopub.status.busy":"2022-03-25T11:01:04.553134Z","iopub.execute_input":"2022-03-25T11:01:04.553703Z","iopub.status.idle":"2022-03-25T11:01:04.559279Z","shell.execute_reply.started":"2022-03-25T11:01:04.553663Z","shell.execute_reply":"2022-03-25T11:01:04.558273Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"def add_margin(pil_img, top, right, bottom, left, color=(255,255,255)):\n    height,width,_ = pil_img.shape\n    new_width = width + right + left\n    new_height = height + top + bottom\n    result = Image.new('RGB', (new_width, new_height), color)\n    result.paste(pil_img, (left, top))\n    return result","metadata":{"execution":{"iopub.status.busy":"2022-03-25T11:01:06.013215Z","iopub.execute_input":"2022-03-25T11:01:06.013502Z","iopub.status.idle":"2022-03-25T11:01:06.019272Z","shell.execute_reply.started":"2022-03-25T11:01:06.013472Z","shell.execute_reply":"2022-03-25T11:01:06.018467Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"def CutLetters(img,rgb_img, count):\n    height, width = img.shape\n    windowWidthR = int(0.18 * width * count)\n    windowWidthL = int(0.12 * width * count)\n    windowWidth = windowWidthR\n    if count == 2:\n        windowWidthR = windowWidthL\n    startRatio = int(0.1 * windowWidth)\n    windowHeight = img.shape[0]\n    marginWidth = int(windowWidth * 0.05)\n    stepSize = 8\n    GroupSize = 8\n    lowerBlackLimit = 0.07\n    upperBlackLimit = 0.9\n\n    Letters = []\n    Letters_imgs = []\n    Letters_imgs_thresh = []\n    for i in range(0, width - (windowWidth), stepSize * GroupSize):\n\n        Group = []\n        for j in range(i, min(i + stepSize * GroupSize, width - (windowWidth)), stepSize):\n            #plt.imshow(img[:, j:j+windowWidth],cmap='gray')\n            #plt.show()\n            if j <= width/2:\n                windowWidth = windowWidthL\n            else:\n                windowWidth = windowWidthR\n\n            blackCountMarginL = marginWidth * height  - np.count_nonzero(img[:, j : j+marginWidth])\n            blackCountMarginR = marginWidth * height  - np.count_nonzero(img[:, j+windowWidth-marginWidth : j+windowWidth])\n            blackCountInner = (windowWidth - 2 * startRatio) * height - np.count_nonzero(img[:, j+startRatio : j+windowWidth-startRatio])\n            \n            #print(blackCountMarginL, marginWidth * height)\n            #print(blackCountMarginR, marginWidth * height)\n            #print(blackCountInner, (windowWidth - 2 * marginWidth) * height)\n            #and blackCountMarginR < 0.2 * marginWidth * height \\\n\n            if blackCountMarginL < 0.17 * marginWidth * height and blackCountMarginR < 0.17 * marginWidth * height and blackCountInner > lowerBlackLimit * (windowWidth - 2 * startRatio) * height \\\n                and blackCountInner < upperBlackLimit * (windowWidth - 2 * startRatio) * height:\n                Group.append((blackCountInner, j))\n        if len(Group) > 0:\n            max_G = max(Group)[1]\n            if len(Letters) == 0 or len(Letters) > 0 and max_G - Letters[-1] > 21:\n                Letters.append(max_G)\n              # rect_coord = [rgb_img[0][0],rgb_img[0][-1],max(Group)[1],max(Group)[1]+windowWidth]\n              # rect_coord = np.array(rect_coord,dtype='int32')\n                rgb2_img = rgb_img[:, max_G:max_G+windowWidth]\n                Letters_imgs.append(rgb2_img)\n                #               plt.imshow(rgb2_img)\n                thresh_img = img[:, max_G:max_G+windowWidth]\n                Letters_imgs_thresh.append(thresh_img)\n              #plt.imshow(img[:, max(Group)[1]:max(Group)[1]+windowWidth],cmap='gray')\n#               plt.show()\n\n\n    return Letters_imgs,Letters_imgs_thresh\n\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-03-25T11:01:07.927215Z","iopub.execute_input":"2022-03-25T11:01:07.927827Z","iopub.status.idle":"2022-03-25T11:01:07.943393Z","shell.execute_reply.started":"2022-03-25T11:01:07.927787Z","shell.execute_reply":"2022-03-25T11:01:07.942469Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"def sideBorder(img, d):\n    h,w=img.shape[0:2]\n    base_size=h+20,w+20,3\n    if d == 1:\n        base_size=h+20,w+20\n    # make a 3 channel image for base which is slightly larger than target img\n    base=np.zeros(base_size,dtype=np.uint8)\n    if d == 1:\n        cv2.rectangle(base,(0,0),(w+20,h+20),255,20) # really thick white rectangle\n    else:\n        cv2.rectangle(base,(0,0),(w+20,h+20),(255,255,255),20) # really thick white rectangle\n    \n    base[10:h+10,10:w+10]=img # this works\n    return base","metadata":{"execution":{"iopub.status.busy":"2022-03-25T11:01:11.042825Z","iopub.execute_input":"2022-03-25T11:01:11.043337Z","iopub.status.idle":"2022-03-25T11:01:11.050490Z","shell.execute_reply.started":"2022-03-25T11:01:11.043281Z","shell.execute_reply":"2022-03-25T11:01:11.049466Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"## Trying to get letters from cropped image, not working yet\ndef CharacterContours(thresh_img):\n    cnts2 = cv2.findContours(thresh_img.copy(), cv2.RETR_EXTERNAL,\n        cv2.CHAIN_APPROX_SIMPLE)\n    cnts2 = imutils.grab_contours(cnts2)\n    cnts2 = sorted(cnts2, key=cv2.contourArea, reverse=True)\n    #cv2.drawContours(thresh_img, cnts, -1, (0, 255, 0), 2)\n    #plt.imshow(thresh_img, cmap=\"gray\")\n    #plt.show()\n\n    letters = []\n    letters_adj = []\n    for c in cnts2:\n        # approximate the contour\n        peri = cv2.arcLength(c, True)\n        approx = cv2.approxPolyDP(c, 0.1 * peri, True)\n        letters.append(approx)\n    #print(letters)\n    #print(len(cnts))\n    #print(len(letters))\n    # for i in range(len(letters)):\n    #     # print(i)\n    #     arr = np.array(letters[i],dtype='int32')\n    #     # arr.reshape(4,2)\n    #     arr = arr.reshape(len(arr[:]),2)\n    #     letters_adj.append(arr)\n    #print(letters_adj[1].reshape(len(letters_adj[1][:]),2))\n    # print(letters_adj[2])\n    # output2 = final_img.copy()\n    # cv2.drawContours(output2, [letters_adj[6]], -1, (0, 255, 0), 2)\n    # plt.imshow(output2)\n    # plt.show()\n    # letters_img = four_point_transform(final_img, letters_adj[1])\n    # plt.imshow(letters_img)\n    # plt.show()\n\n","metadata":{"execution":{"iopub.status.busy":"2022-03-25T11:01:15.796777Z","iopub.execute_input":"2022-03-25T11:01:15.798959Z","iopub.status.idle":"2022-03-25T11:01:15.805589Z","shell.execute_reply.started":"2022-03-25T11:01:15.798920Z","shell.execute_reply":"2022-03-25T11:01:15.804564Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"code","source":"for i in range(30):\n    imgOriginal = cv2.imread(f\"../input/testdata/test_images/{str(i).zfill(5)}.jpg\")\n    #imgOriginal = cv2.imread(\"./train_images/02937.jpg\")\n    #img = imgOriginal.copy()\n    imgOriginal = imutils.resize(imgOriginal, width=500)\n    #ratio = img.shape[1] / float(imgOriginal.shape[1])\n#     plt.imshow(imgOriginal)\n#     plt.show()\n    gray,thresh = preprocess(imgOriginal)\n    # plt.imshow(gray,cmap='gray')\n    # plt.show()\n#     plt.imshow(thresh,cmap='gray')\n#     plt.show()\n\n    #ClosedThresh = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, cv2.getStructuringElement(cv2.MORPH_RECT, (15, 3)))\n    OpenedThresh = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, cv2.getStructuringElement(cv2.MORPH_RECT, (7, 7)))\n\n    minValue = np.min(OpenedThresh)\n\n    OpenedThresh[imgOriginal.shape[0]-4:imgOriginal.shape[0]-1, :] = minValue\n    OpenedThresh[:, imgOriginal.shape[1]-4:imgOriginal.shape[1]-1] = minValue\n    OpenedThresh[:, 0:3] = minValue\n\n\n#     plt.imshow(OpenedThresh,cmap='gray')\n#     plt.show()\n\n\n\n    #plt.imshow(ClosedThresh,cmap='gray')\n    #plt.show()\n    \n\n    cnts = cv2.findContours(OpenedThresh.copy(), cv2.RETR_EXTERNAL,\n        cv2.CHAIN_APPROX_SIMPLE)\n    cnts = imutils.grab_contours(cnts)\n    cnts = sorted(cnts, key=cv2.contourArea, reverse=True)\n    # for c in cnts:\n        # #print(c[0])\n        # plt.scatter(c[:, 0, 0], -c[:, 0, 1])\n        # plt.show()\n    # np.array(cnts,dtype=np.int32).shape\n    #print(cnts[-1])\n    receiptCnt = contourApprox(imgOriginal, cnts)\n    if len(receiptCnt) == 0:\n        \n        receiptCnt = [np.array([[[0,  0]], [[0, imgOriginal.shape[0]]],\n         [[imgOriginal.shape[1], imgOriginal.shape[0]]], [[imgOriginal.shape[1], 0]]])]\n    #print(receiptCnt)\n\n    for r in receiptCnt:\n\n        final_img ,rgb_img= showApprox(imgOriginal, thresh, r)\n        final_img = sideBorder(final_img, 1)\n        rgb_img = sideBorder(rgb_img, 3)\n#         plt.imshow(rgb_img)\n#         plt.show()\n#         plt.imshow(final_img,cmap='gray')\n#         plt.show()\n        #resized_img = imutils.resize(final_img[:,:int(final_img.shape[1]/2)], width=int(1.5 * final_img.shape[1]/2))\n        #new_img = np.concatenate((resized_img,final_img[:,int(final_img.shape[1]/2):]),axis=1)\n        #print(f\"index = {int(final_img.shape[1]/2)}\")\n\n        #print(resized_img.shape[1])\n        # new_im = Image.new('RGB', (resized_img.shape[0]+final_img[:,int(final_img.shape[1]/2):].shape[1],\n        # max(resized_img.shape[0],final_img.shape[0])), (250,250,250))\n\n        # new_im.paste(resized_img, (0,0))\n        # new_im.paste(final_img[:,int(final_img.shape[1]/2):], (resized_img.shape[1],0))\n        # plt.imshow(new_im)\n        # plt.show()\n        #CutLetters(rgb_img)\n\n        letters_cropped, letters_cropped_thresh = CutLetters(final_img,rgb_img, len(receiptCnt))\n        for character in letters_cropped_thresh:\n            charcater = np.array(character)\n            plt.imshow(charcater)\n            plt.show()\n            print(predict(img=character, model=model, labels=labels))\n          #print(final_img.shape[1])\n          #CharacterContours(final_img)\n\n# plt.plot(cnts[0])\n# plt.show()\n\n#cnts = skimage.measure.find_contours(thresh)\n#cnts = sorted(cnts, reverse=True)\n#cnts\n\n# for c in cnts:\n#     # compute the bounding box of the contour and then use\n#     # the bounding box to derive the aspect ratio\n#     (x, y, w, h) = cv2.boundingRect(c)\n#     ar = w / float(h)\n#     if ar >= 2 and ar <= 10:\n#         lpCnt = c\n#         letter = gray[y:y + h, x:x + w]\n#         roi = cv2.threshold(letter, 0, 255,\n#                 cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]\n#             # check to see if we should clear any foreground\n#             # pixels touching the border of the image\n#             # (which typically, not but always, indicates noise)\n#         # display any debugging information and then break\n#         # from the loop early since we have found the license\n#         # plate region\n#         # plt.plot(letter,cmap='gray')\n#         # plt.show()\n#         plt.plot(roi,cmap='gray')\n#         plt.show()\n#         print(\"****************\")\n# print(roi)   ","metadata":{"execution":{"iopub.status.busy":"2022-03-25T11:01:57.714429Z","iopub.execute_input":"2022-03-25T11:01:57.714759Z","iopub.status.idle":"2022-03-25T11:02:31.715424Z","shell.execute_reply.started":"2022-03-25T11:01:57.714724Z","shell.execute_reply":"2022-03-25T11:02:31.714293Z"},"trusted":true},"execution_count":82,"outputs":[]}]}